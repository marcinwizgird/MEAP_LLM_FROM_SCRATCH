{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-31T10:49:08.977347Z",
     "start_time": "2025-12-31T10:49:05.646979Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Silicon GPU (Metal)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "batch_size = 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "embeddings = torch.randn((batch_size, context_len, embed_dim), device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.0+cu130\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Casual MHA Attention Wrapper",
   "id": "a42f23ccc0f70153"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T10:49:11.277206Z",
     "start_time": "2025-12-31T10:49:10.507588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)  # New\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))  # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class Ch03_MHA_Wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.out_proj(context_vec)\n",
    "\n",
    "\n",
    "mha_ch03_wrapper = Ch03_MHA_Wrapper(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim//12,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_ch03_wrapper(embeddings)\n",
    "print(out.shape)"
   ],
   "id": "b8f173ee25dbb25e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Final Course MHA Implementation",
   "id": "6aa332726e1853e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:47:46.374908Z",
     "start_time": "2025-11-23T11:47:46.319604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Ch03_MHA(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "mha_ch03 = Ch03_MHA(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "out = mha_ch03(embeddings)\n",
    "print(out.shape)"
   ],
   "id": "911e7d28231104c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MHA with Cpmbined Weights",
   "id": "306b9233fd31e428"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:55:10.451953Z",
     "start_time": "2025-11-23T11:55:09.132764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MHAv_2_0(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.WQKV = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        qkv = self.WQKV(x)  # Shape: (b, num_tokens, 3 * d_out)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, 3 * d_out) -> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(b, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        #Unbinding qvk into queries, keys and values:\n",
    "        #(b, num_heads, num_tokens, 3 * head_dim) ->(3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        queries, keys, values = qkv.unbind(dim = 0)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "mha_v_2_0 = MHAv_2_0(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out_mha_v_2_0 = mha_v_2_0(embeddings)\n",
    "print(out.shape)"
   ],
   "id": "8944470dff50268c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:55:11.598151Z",
     "start_time": "2025-11-23T11:55:11.536919Z"
    }
   },
   "cell_type": "code",
   "source": "out_mha_v_2_0",
   "id": "668198f9eb4b722a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3466,  0.1000, -0.1979,  ...,  0.2440,  0.0190,  0.4200],\n",
       "         [-0.2110,  0.0193, -0.3820,  ...,  0.0796,  0.0849,  0.1457],\n",
       "         [ 0.1017, -0.0742, -0.2684,  ...,  0.1443,  0.1109,  0.1662],\n",
       "         ...,\n",
       "         [ 0.0252, -0.0407, -0.0086,  ...,  0.0308, -0.0259,  0.0387],\n",
       "         [ 0.0216, -0.0290, -0.0182,  ...,  0.0261, -0.0294,  0.0390],\n",
       "         [ 0.0297, -0.0350, -0.0167,  ...,  0.0406, -0.0295,  0.0408]],\n",
       "\n",
       "        [[ 0.2520,  0.4761, -0.4031,  ...,  0.2052,  0.3203,  0.2161],\n",
       "         [ 0.1707,  0.2257,  0.0142,  ...,  0.0013,  0.2052, -0.0628],\n",
       "         [ 0.0692,  0.2422, -0.1263,  ..., -0.0167, -0.0215, -0.1381],\n",
       "         ...,\n",
       "         [ 0.0351, -0.0340, -0.0125,  ...,  0.0210, -0.0496,  0.0555],\n",
       "         [ 0.0469, -0.0355, -0.0117,  ...,  0.0182, -0.0533,  0.0542],\n",
       "         [ 0.0301, -0.0378, -0.0109,  ...,  0.0235, -0.0519,  0.0561]],\n",
       "\n",
       "        [[ 0.0508, -0.1723, -0.2799,  ...,  0.2172, -0.2496, -0.2822],\n",
       "         [ 0.0254, -0.4323, -0.1117,  ..., -0.1611, -0.0107, -0.0418],\n",
       "         [ 0.1505, -0.4707, -0.1327,  ...,  0.1048,  0.0298,  0.0254],\n",
       "         ...,\n",
       "         [ 0.0423, -0.0298,  0.0011,  ...,  0.0313, -0.0337,  0.0325],\n",
       "         [ 0.0447, -0.0278, -0.0022,  ...,  0.0433, -0.0446,  0.0362],\n",
       "         [ 0.0483, -0.0430, -0.0115,  ...,  0.0456, -0.0389,  0.0279]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5230, -0.0955, -0.1564,  ..., -0.3256, -0.4700, -0.4507],\n",
       "         [ 0.3085,  0.3226, -0.2677,  ..., -0.0517, -0.2753, -0.0500],\n",
       "         [ 0.1175,  0.2562, -0.1252,  ..., -0.0713, -0.0445, -0.1358],\n",
       "         ...,\n",
       "         [ 0.0386, -0.0183, -0.0093,  ...,  0.0354, -0.0055,  0.0560],\n",
       "         [ 0.0445, -0.0029, -0.0077,  ...,  0.0314, -0.0065,  0.0567],\n",
       "         [ 0.0376, -0.0198,  0.0081,  ...,  0.0300, -0.0149,  0.0596]],\n",
       "\n",
       "        [[ 0.1469, -0.4170, -0.0456,  ...,  0.4647,  0.1001,  0.1017],\n",
       "         [ 0.2138, -0.1039,  0.2662,  ...,  0.3258,  0.1168, -0.1407],\n",
       "         [ 0.3042, -0.0120,  0.2192,  ...,  0.3538, -0.0077,  0.0133],\n",
       "         ...,\n",
       "         [ 0.0272, -0.0305, -0.0160,  ...,  0.0421, -0.0273,  0.0189],\n",
       "         [ 0.0259, -0.0374, -0.0189,  ...,  0.0290, -0.0246,  0.0158],\n",
       "         [ 0.0205, -0.0208, -0.0189,  ...,  0.0298, -0.0327,  0.0270]],\n",
       "\n",
       "        [[ 0.0032, -0.3890, -0.0152,  ..., -0.3226,  0.2426,  0.3340],\n",
       "         [ 0.0582, -0.2846, -0.1989,  ..., -0.2576, -0.1341, -0.0477],\n",
       "         [-0.1261, -0.1685, -0.1844,  ..., -0.2260, -0.2420,  0.0874],\n",
       "         ...,\n",
       "         [ 0.0314, -0.0345, -0.0110,  ...,  0.0256, -0.0452,  0.0331],\n",
       "         [ 0.0323, -0.0315, -0.0131,  ...,  0.0123, -0.0485,  0.0336],\n",
       "         [ 0.0301, -0.0282, -0.0084,  ...,  0.0183, -0.0479,  0.0400]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MHA with Pytorch einsum notation",
   "id": "c7451a5b186fce3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:31:47.919055Z",
     "start_time": "2025-12-31T11:31:47.873704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class MHAwithEinsum(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Parameter(torch.randn(d_out, d_in))\n",
    "        self.W_key = nn.Parameter(torch.randn(d_out, d_in))\n",
    "        self.W_value = nn.Parameter(torch.randn(d_out, d_in))\n",
    "\n",
    "        if qkv_bias:\n",
    "            self.bias_q = nn.Parameter(torch.zeros(d_out))\n",
    "            self.bias_k = nn.Parameter(torch.zeros(d_out))\n",
    "            self.bias_v = nn.Parameter(torch.zeros(d_out))\n",
    "        else:\n",
    "            self.register_parameter(\"bias_q\", None)\n",
    "            self.register_parameter(\"bias_k\", None)\n",
    "            self.register_parameter(\"bias_v\", None)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_query, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_key, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_value, a=math.sqrt(5))\n",
    "        if self.bias_q is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_query)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias_q, -bound, bound)\n",
    "            nn.init.uniform_(self.bias_k, -bound, bound)\n",
    "            nn.init.uniform_(self.bias_v, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = torch.einsum(\"bij,jk->bik\", x, self.W_key)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = torch.einsum(\"bij,jk->bik\", x, self.W_query)\n",
    "        values = torch.einsum(\"bij,jk->bik\", x, self.W_value)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        #attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        attn_scores = torch.einsum(\"bhil,bhkl->bhik\", queries, values)\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        #context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = torch.einsum(\"bhij,bhjk->bhik\", attn_weights, values)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "        #context_vec = torch.einsum(\"ij,bjk->bik\", self.out_proj.weight, context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "mha_einsum = MHAwithEinsum(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "out_einsum = mha_einsum(embeddings)\n",
    "print(out.shape)"
   ],
   "id": "fd1adf303f464a2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Outputs will be different due to the different initialization WQ, WK, WV matrices compared to the nn.Linear default initialization",
   "id": "b526835fb76e9508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:31:55.131896Z",
     "start_time": "2025-12-31T11:31:55.097420Z"
    }
   },
   "cell_type": "code",
   "source": "out_einsum",
   "id": "9babf3a45f9d7de9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2838e+01, -4.2225e+00,  1.0345e+01,  ..., -5.1643e+00,\n",
       "          -1.7280e+01,  1.1041e+01],\n",
       "         [-5.9008e+00, -1.3893e+01,  3.2544e+00,  ...,  2.1069e+01,\n",
       "          -1.9103e+01, -3.4058e+01],\n",
       "         [-5.9609e+00, -2.3658e+00,  3.3571e+01,  ..., -2.7253e+01,\n",
       "           1.4610e+01,  1.1935e+01],\n",
       "         ...,\n",
       "         [-2.7819e+01, -8.1491e+00, -2.0052e+01,  ..., -2.3420e+01,\n",
       "          -8.2441e+00, -6.7611e+00],\n",
       "         [ 5.8565e+00,  5.0450e+00, -9.8521e+00,  ...,  4.5243e-01,\n",
       "           1.9892e+01, -1.8916e+01],\n",
       "         [-1.1059e+00, -1.2981e+01, -1.2073e+01,  ...,  1.8027e+01,\n",
       "          -1.1175e+01,  5.1175e+00]],\n",
       "\n",
       "        [[ 1.6513e+01, -1.3157e+01, -1.2579e+01,  ..., -7.4693e+00,\n",
       "          -8.2072e+00,  1.0994e+01],\n",
       "         [-2.1182e+01,  8.3377e+00, -4.7878e+00,  ..., -1.1999e+01,\n",
       "          -3.0398e+01, -3.5175e+00],\n",
       "         [ 4.4362e+00, -8.5555e-02,  3.5119e+01,  ...,  1.9740e+01,\n",
       "          -1.8248e+01, -9.9860e+00],\n",
       "         ...,\n",
       "         [-1.1853e+01,  1.6808e+00, -2.0654e+01,  ...,  3.0523e+01,\n",
       "           5.0553e+00, -8.7223e+00],\n",
       "         [ 8.9370e+00,  1.4917e+01,  1.8675e+01,  ..., -6.3321e+00,\n",
       "          -7.5435e+00,  2.7835e+01],\n",
       "         [-7.2197e+00,  2.4576e+00, -5.4353e+00,  ...,  1.5982e+01,\n",
       "           1.9440e+00, -2.2036e+01]],\n",
       "\n",
       "        [[ 5.8607e+00, -3.5504e+01, -2.8439e+01,  ...,  5.9192e+00,\n",
       "          -3.5341e+01,  2.5834e+01],\n",
       "         [ 5.1051e+00, -1.5357e+01,  5.9096e+00,  ..., -1.3264e+01,\n",
       "          -1.8462e+00,  3.4347e+01],\n",
       "         [ 3.3848e+00,  1.8400e+00,  1.5231e+01,  ...,  1.8012e+01,\n",
       "           1.8848e+01,  1.2844e+00],\n",
       "         ...,\n",
       "         [ 8.5406e+00,  4.2386e+01, -3.5908e+01,  ..., -1.9876e+01,\n",
       "           2.1580e+01,  9.6125e+00],\n",
       "         [-1.2887e+01, -1.8365e+01, -9.7391e+00,  ...,  2.1899e+01,\n",
       "          -7.7660e-04,  5.0885e-02],\n",
       "         [-3.1718e+01,  1.1002e+00,  2.7464e+01,  ..., -1.7615e+01,\n",
       "          -7.3447e+00, -1.0815e+01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.6002e+00, -1.2280e+01,  1.4775e+01,  ..., -6.1141e+00,\n",
       "          -5.6247e+00,  8.9834e+00],\n",
       "         [-9.2566e+00, -2.3248e+01,  1.0147e+00,  ..., -3.7700e+00,\n",
       "           1.8524e+00,  2.5094e+00],\n",
       "         [ 6.1908e+00, -4.4723e+00, -2.2149e+01,  ...,  1.0217e+01,\n",
       "           3.3056e+01, -1.5764e+01],\n",
       "         ...,\n",
       "         [-1.1663e+01, -2.8858e+01, -4.6294e-01,  ...,  5.2776e+00,\n",
       "          -3.0440e+01, -1.9072e+00],\n",
       "         [ 1.3741e+01, -6.0544e+00,  9.5625e-01,  ..., -1.2239e+01,\n",
       "          -5.9617e+00, -3.2886e+01],\n",
       "         [ 4.0970e+00, -3.5248e+01, -1.4571e+01,  ...,  3.4130e+00,\n",
       "          -3.7992e+01,  7.3331e+00]],\n",
       "\n",
       "        [[ 1.5453e+01, -1.6715e+01,  1.8877e+00,  ...,  1.4176e+01,\n",
       "          -1.3144e+01, -3.1121e+01],\n",
       "         [ 1.1400e+01,  4.7671e+00, -1.3739e+01,  ..., -5.2940e+00,\n",
       "          -1.6705e+01,  6.5150e+00],\n",
       "         [-1.5672e+01,  1.5739e+01, -5.6878e+00,  ...,  2.4561e+01,\n",
       "          -2.0225e+01,  1.7373e+01],\n",
       "         ...,\n",
       "         [ 3.0545e+01, -1.4040e+01,  2.6038e+00,  ...,  1.6271e+01,\n",
       "           1.5856e+01, -2.0695e+01],\n",
       "         [ 2.3293e+01,  1.9293e+01,  1.4387e+01,  ...,  4.5430e+00,\n",
       "          -2.6948e+00, -3.2137e+00],\n",
       "         [ 2.1253e+01,  5.0246e-01, -1.5856e+00,  ...,  1.2550e+01,\n",
       "           4.3695e+01,  1.4090e+01]],\n",
       "\n",
       "        [[-7.0254e+00, -2.7633e+01,  1.1110e+00,  ...,  4.0681e+00,\n",
       "          -3.1304e+01,  1.7253e+00],\n",
       "         [-1.2488e+01, -2.1877e+01, -4.3078e+00,  ..., -3.9431e+01,\n",
       "           7.5158e+00,  1.2847e+01],\n",
       "         [ 1.0600e+00, -2.9200e+01,  2.9983e-01,  ..., -5.2891e+00,\n",
       "           5.2204e+00, -2.8827e+01],\n",
       "         ...,\n",
       "         [ 2.9815e+01,  1.0546e+01,  9.4734e+00,  ...,  1.3847e+01,\n",
       "          -2.1642e+01,  5.3375e+00],\n",
       "         [ 1.9611e+01, -2.8546e+01, -1.6508e+01,  ..., -8.1080e+00,\n",
       "           8.7384e-01,  1.7315e+01],\n",
       "         [ 1.5765e+01, -3.8385e+00, -2.0983e+01,  ...,  2.0871e+01,\n",
       "          -4.0343e+00, -1.1019e+01]]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5f5b146777a9d64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
