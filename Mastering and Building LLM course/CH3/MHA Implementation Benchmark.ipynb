{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T09:34:03.286041Z",
     "start_time": "2025-11-23T09:34:03.258578Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Silicon GPU (Metal)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "batch_size = 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "embeddings = torch.randn((batch_size, context_len, embed_dim), device=device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.0+cu130\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Casual MHA Attention Wrapper",
   "id": "a42f23ccc0f70153"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T09:34:42.859256Z",
     "start_time": "2025-11-23T09:34:40.758909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)  # New\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))  # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class Ch03_MHA_Wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.out_proj(context_vec)\n",
    "\n",
    "\n",
    "mha_ch03_wrapper = Ch03_MHA_Wrapper(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim//12,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out = mha_ch03_wrapper(embeddings)\n",
    "print(out.shape)"
   ],
   "id": "b8f173ee25dbb25e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Final Course MHA Implementation",
   "id": "6aa332726e1853e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:47:46.374908Z",
     "start_time": "2025-11-23T11:47:46.319604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Ch03_MHA(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "mha_ch03 = Ch03_MHA(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "out = mha_ch03(embeddings)\n",
    "print(out.shape)"
   ],
   "id": "911e7d28231104c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:47:47.471466Z",
     "start_time": "2025-11-23T11:47:47.418182Z"
    }
   },
   "cell_type": "code",
   "source": "out",
   "id": "658771005f19718a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3466,  0.1000, -0.1979,  ...,  0.2440,  0.0190,  0.4200],\n",
       "         [-0.2110,  0.0193, -0.3820,  ...,  0.0796,  0.0849,  0.1457],\n",
       "         [ 0.1017, -0.0742, -0.2684,  ...,  0.1443,  0.1109,  0.1662],\n",
       "         ...,\n",
       "         [ 0.0252, -0.0407, -0.0086,  ...,  0.0308, -0.0259,  0.0387],\n",
       "         [ 0.0216, -0.0290, -0.0182,  ...,  0.0261, -0.0294,  0.0390],\n",
       "         [ 0.0297, -0.0350, -0.0167,  ...,  0.0406, -0.0295,  0.0408]],\n",
       "\n",
       "        [[ 0.2520,  0.4761, -0.4031,  ...,  0.2052,  0.3203,  0.2161],\n",
       "         [ 0.1707,  0.2257,  0.0142,  ...,  0.0013,  0.2052, -0.0628],\n",
       "         [ 0.0692,  0.2422, -0.1263,  ..., -0.0167, -0.0215, -0.1381],\n",
       "         ...,\n",
       "         [ 0.0351, -0.0340, -0.0125,  ...,  0.0210, -0.0496,  0.0555],\n",
       "         [ 0.0469, -0.0355, -0.0117,  ...,  0.0182, -0.0533,  0.0542],\n",
       "         [ 0.0301, -0.0378, -0.0109,  ...,  0.0235, -0.0519,  0.0561]],\n",
       "\n",
       "        [[ 0.0508, -0.1723, -0.2799,  ...,  0.2172, -0.2496, -0.2822],\n",
       "         [ 0.0254, -0.4323, -0.1117,  ..., -0.1611, -0.0107, -0.0418],\n",
       "         [ 0.1505, -0.4707, -0.1327,  ...,  0.1048,  0.0298,  0.0254],\n",
       "         ...,\n",
       "         [ 0.0423, -0.0298,  0.0011,  ...,  0.0313, -0.0337,  0.0325],\n",
       "         [ 0.0447, -0.0278, -0.0022,  ...,  0.0433, -0.0446,  0.0362],\n",
       "         [ 0.0483, -0.0430, -0.0115,  ...,  0.0456, -0.0389,  0.0279]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5230, -0.0955, -0.1564,  ..., -0.3256, -0.4700, -0.4507],\n",
       "         [ 0.3085,  0.3226, -0.2677,  ..., -0.0517, -0.2753, -0.0500],\n",
       "         [ 0.1175,  0.2562, -0.1252,  ..., -0.0713, -0.0445, -0.1358],\n",
       "         ...,\n",
       "         [ 0.0386, -0.0183, -0.0093,  ...,  0.0354, -0.0055,  0.0560],\n",
       "         [ 0.0445, -0.0029, -0.0077,  ...,  0.0314, -0.0065,  0.0567],\n",
       "         [ 0.0376, -0.0198,  0.0081,  ...,  0.0300, -0.0149,  0.0596]],\n",
       "\n",
       "        [[ 0.1469, -0.4170, -0.0456,  ...,  0.4647,  0.1001,  0.1017],\n",
       "         [ 0.2138, -0.1039,  0.2662,  ...,  0.3258,  0.1168, -0.1407],\n",
       "         [ 0.3042, -0.0120,  0.2192,  ...,  0.3538, -0.0077,  0.0133],\n",
       "         ...,\n",
       "         [ 0.0272, -0.0305, -0.0160,  ...,  0.0421, -0.0273,  0.0189],\n",
       "         [ 0.0259, -0.0374, -0.0189,  ...,  0.0290, -0.0246,  0.0158],\n",
       "         [ 0.0205, -0.0208, -0.0189,  ...,  0.0298, -0.0327,  0.0270]],\n",
       "\n",
       "        [[ 0.0032, -0.3890, -0.0152,  ..., -0.3226,  0.2426,  0.3340],\n",
       "         [ 0.0582, -0.2846, -0.1989,  ..., -0.2576, -0.1341, -0.0477],\n",
       "         [-0.1261, -0.1685, -0.1844,  ..., -0.2260, -0.2420,  0.0874],\n",
       "         ...,\n",
       "         [ 0.0314, -0.0345, -0.0110,  ...,  0.0256, -0.0452,  0.0331],\n",
       "         [ 0.0323, -0.0315, -0.0131,  ...,  0.0123, -0.0485,  0.0336],\n",
       "         [ 0.0301, -0.0282, -0.0084,  ...,  0.0183, -0.0479,  0.0400]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MHA with Cpmbined Weights",
   "id": "306b9233fd31e428"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:55:10.451953Z",
     "start_time": "2025-11-23T11:55:09.132764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MHAv_2_0(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.WQKV = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        qkv = self.WQKV(x)  # Shape: (b, num_tokens, 3 * d_out)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, 3 * d_out) -> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(b, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        #Unbinding qvk into queries, keys and values:\n",
    "        #(b, num_heads, num_tokens, 3 * head_dim) ->(3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        queries, keys, values = qkv.unbind(dim = 0)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "mha_v_2_0 = MHAv_2_0(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=12,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "out_mha_v_2_0 = mha_v_2_0(embeddings)\n",
    "print(out.shape)"
   ],
   "id": "8944470dff50268c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:55:11.598151Z",
     "start_time": "2025-11-23T11:55:11.536919Z"
    }
   },
   "cell_type": "code",
   "source": "out_mha_v_2_0",
   "id": "668198f9eb4b722a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3466,  0.1000, -0.1979,  ...,  0.2440,  0.0190,  0.4200],\n",
       "         [-0.2110,  0.0193, -0.3820,  ...,  0.0796,  0.0849,  0.1457],\n",
       "         [ 0.1017, -0.0742, -0.2684,  ...,  0.1443,  0.1109,  0.1662],\n",
       "         ...,\n",
       "         [ 0.0252, -0.0407, -0.0086,  ...,  0.0308, -0.0259,  0.0387],\n",
       "         [ 0.0216, -0.0290, -0.0182,  ...,  0.0261, -0.0294,  0.0390],\n",
       "         [ 0.0297, -0.0350, -0.0167,  ...,  0.0406, -0.0295,  0.0408]],\n",
       "\n",
       "        [[ 0.2520,  0.4761, -0.4031,  ...,  0.2052,  0.3203,  0.2161],\n",
       "         [ 0.1707,  0.2257,  0.0142,  ...,  0.0013,  0.2052, -0.0628],\n",
       "         [ 0.0692,  0.2422, -0.1263,  ..., -0.0167, -0.0215, -0.1381],\n",
       "         ...,\n",
       "         [ 0.0351, -0.0340, -0.0125,  ...,  0.0210, -0.0496,  0.0555],\n",
       "         [ 0.0469, -0.0355, -0.0117,  ...,  0.0182, -0.0533,  0.0542],\n",
       "         [ 0.0301, -0.0378, -0.0109,  ...,  0.0235, -0.0519,  0.0561]],\n",
       "\n",
       "        [[ 0.0508, -0.1723, -0.2799,  ...,  0.2172, -0.2496, -0.2822],\n",
       "         [ 0.0254, -0.4323, -0.1117,  ..., -0.1611, -0.0107, -0.0418],\n",
       "         [ 0.1505, -0.4707, -0.1327,  ...,  0.1048,  0.0298,  0.0254],\n",
       "         ...,\n",
       "         [ 0.0423, -0.0298,  0.0011,  ...,  0.0313, -0.0337,  0.0325],\n",
       "         [ 0.0447, -0.0278, -0.0022,  ...,  0.0433, -0.0446,  0.0362],\n",
       "         [ 0.0483, -0.0430, -0.0115,  ...,  0.0456, -0.0389,  0.0279]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5230, -0.0955, -0.1564,  ..., -0.3256, -0.4700, -0.4507],\n",
       "         [ 0.3085,  0.3226, -0.2677,  ..., -0.0517, -0.2753, -0.0500],\n",
       "         [ 0.1175,  0.2562, -0.1252,  ..., -0.0713, -0.0445, -0.1358],\n",
       "         ...,\n",
       "         [ 0.0386, -0.0183, -0.0093,  ...,  0.0354, -0.0055,  0.0560],\n",
       "         [ 0.0445, -0.0029, -0.0077,  ...,  0.0314, -0.0065,  0.0567],\n",
       "         [ 0.0376, -0.0198,  0.0081,  ...,  0.0300, -0.0149,  0.0596]],\n",
       "\n",
       "        [[ 0.1469, -0.4170, -0.0456,  ...,  0.4647,  0.1001,  0.1017],\n",
       "         [ 0.2138, -0.1039,  0.2662,  ...,  0.3258,  0.1168, -0.1407],\n",
       "         [ 0.3042, -0.0120,  0.2192,  ...,  0.3538, -0.0077,  0.0133],\n",
       "         ...,\n",
       "         [ 0.0272, -0.0305, -0.0160,  ...,  0.0421, -0.0273,  0.0189],\n",
       "         [ 0.0259, -0.0374, -0.0189,  ...,  0.0290, -0.0246,  0.0158],\n",
       "         [ 0.0205, -0.0208, -0.0189,  ...,  0.0298, -0.0327,  0.0270]],\n",
       "\n",
       "        [[ 0.0032, -0.3890, -0.0152,  ..., -0.3226,  0.2426,  0.3340],\n",
       "         [ 0.0582, -0.2846, -0.1989,  ..., -0.2576, -0.1341, -0.0477],\n",
       "         [-0.1261, -0.1685, -0.1844,  ..., -0.2260, -0.2420,  0.0874],\n",
       "         ...,\n",
       "         [ 0.0314, -0.0345, -0.0110,  ...,  0.0256, -0.0452,  0.0331],\n",
       "         [ 0.0323, -0.0315, -0.0131,  ...,  0.0123, -0.0485,  0.0336],\n",
       "         [ 0.0301, -0.0282, -0.0084,  ...,  0.0183, -0.0479,  0.0400]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "309fc13e537de2a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
