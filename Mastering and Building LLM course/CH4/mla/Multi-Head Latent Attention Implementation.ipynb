{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-26T18:53:18.477638Z",
     "start_time": "2026-02-26T18:53:07.638486Z"
    }
   },
   "source": [
    "import argparse\n",
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T18:53:18.494615Z",
     "start_time": "2026-02-26T18:53:18.488530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False,        # Query-Key-Value bias\n",
    "        \"latent_dim\" : 16\n",
    "    }"
   ],
   "id": "432febf43fcad6ba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T09:56:36.391057Z",
     "start_time": "2026-02-27T09:56:36.368291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False, latent_dim=None):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "        self.latent_dim = latent_dim if latent_dim is not None else max(16, d_out // 8)\n",
    "\n",
    "        self.W_query =  nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_DKV = nn.Linear(d_in, self.latent_dim, bias=qkv_bias)\n",
    "        self.W_UK = nn.Linear(self.latent_dim, self.d_out, bias=qkv_bias)\n",
    "        self.W_UV = nn.Linear(self.latent_dim, self.d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ####################################################\n",
    "        # KV cache-related code\n",
    "        self.register_buffer(\"cache_c_kv\", None, persistent=False)\n",
    "        self.ptr_current_pos = 0\n",
    "        ####################################################\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        x_udv = None\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        x_udv_new = self.W_DKV(x)\n",
    "\n",
    "        ####################################################\n",
    "        # KV cache-related\n",
    "        if use_cache:\n",
    "            if self.cache_c_kv is None:\n",
    "                self.cache_c_kv = x_udv_new\n",
    "            else:\n",
    "                self.cache_c_kv = torch.cat([self.cache_c_kv, x_udv_new], dim=1)\n",
    "\n",
    "            x_udv = self.cache_c_kv\n",
    "        else:\n",
    "            x_udv = x_udv_new\n",
    "        ####################################################\n",
    "\n",
    "        keys = self.W_UK(x_udv)  # Shape: (b, seq_length, d_out)\n",
    "        values = self.W_UV(x_udv) # Shape: (b, seq_length, d_out)\n",
    "        queries = self.W_query(x) # Shape: (b, num_tokens, d_out)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        _, seq_length, _ = keys.shape\n",
    "        keys = keys.view(b, seq_length, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        values = values.view(b, seq_length, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        ########################################################################################\n",
    "        #NEW\n",
    "\n",
    "        Q_num_tokens = queries.shape[-2]\n",
    "        K_num_tokens = keys.shape[-2]\n",
    "\n",
    "        device = queries.device\n",
    "        if use_cache:\n",
    "            q_positions = torch.arange(\n",
    "                self.ptr_current_pos,\n",
    "                self.ptr_current_pos + Q_num_tokens,\n",
    "                device=device,\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "            self.ptr_current_pos += Q_num_tokens\n",
    "        else:\n",
    "            q_positions = torch.arange(Q_num_tokens, device=device, dtype=torch.long)\n",
    "            self.ptr_current_pos = 0\n",
    "        k_positions = torch.arange(K_num_tokens, device=device, dtype=torch.long)\n",
    "        mask_bool = q_positions.unsqueeze(-1) < k_positions.unsqueeze(0)\n",
    "        ########################################################################################\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "    def reset_cache(self):\n",
    "        self.cache_c_kv = None\n",
    "        self.ptr_current_pos = 0"
   ],
   "id": "a0e8940117ee637c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T09:59:05.646965Z",
     "start_time": "2026-02-27T09:59:05.599447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "mhla = MultiHeadLatentAttention(768, 768, 0.1, 8, latent_dim = 16)\n",
    "x = torch.randn((4, 16, 768))\n",
    "mhla(x, use_cache=False)"
   ],
   "id": "ce57bf9c7795ac26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4875, -0.0411,  0.0817,  ...,  0.4159,  0.0751, -0.0362],\n",
       "         [-0.3747,  0.0040, -0.1376,  ...,  0.1695,  0.0344,  0.1456],\n",
       "         [-0.2558, -0.0190, -0.0046,  ..., -0.1732, -0.0314,  0.0327],\n",
       "         ...,\n",
       "         [-0.0434, -0.0430,  0.0536,  ..., -0.0190,  0.1086,  0.0185],\n",
       "         [-0.0076, -0.0088,  0.0619,  ...,  0.0151,  0.0752,  0.0256],\n",
       "         [-0.0299, -0.0438,  0.0467,  ..., -0.0277,  0.0474,  0.0263]],\n",
       "\n",
       "        [[-0.1220,  0.0608,  0.0130,  ..., -0.1628,  0.1217,  0.1792],\n",
       "         [-0.2831,  0.0726,  0.0782,  ...,  0.1233,  0.1931,  0.0833],\n",
       "         [-0.1875,  0.0378,  0.0440,  ...,  0.1351,  0.1351,  0.0959],\n",
       "         ...,\n",
       "         [ 0.0017,  0.0045, -0.0118,  ...,  0.0411,  0.0340,  0.0081],\n",
       "         [-0.0067,  0.0040,  0.0021,  ...,  0.0042,  0.0231, -0.0021],\n",
       "         [ 0.0036,  0.0009, -0.0563,  ...,  0.0039,  0.0498, -0.0169]],\n",
       "\n",
       "        [[ 0.0805,  0.0756, -0.2643,  ..., -0.1146,  0.0218,  0.1169],\n",
       "         [ 0.0141,  0.1417, -0.2870,  ...,  0.0772, -0.1675,  0.0676],\n",
       "         [-0.0129,  0.0579, -0.0195,  ..., -0.0410, -0.0302,  0.1583],\n",
       "         ...,\n",
       "         [ 0.0503,  0.0310, -0.0494,  ..., -0.0887,  0.0247,  0.0326],\n",
       "         [ 0.0169,  0.0429, -0.0409,  ..., -0.0932,  0.0406,  0.0613],\n",
       "         [ 0.0546,  0.0274, -0.0483,  ..., -0.0359,  0.0289,  0.0662]],\n",
       "\n",
       "        [[ 0.4103,  0.1271,  0.1097,  ..., -0.2564,  0.0220, -0.0653],\n",
       "         [ 0.1611,  0.0983,  0.2950,  ..., -0.0189,  0.3222, -0.0756],\n",
       "         [ 0.0055,  0.0563,  0.2137,  ..., -0.0778,  0.2371, -0.0616],\n",
       "         ...,\n",
       "         [ 0.0220,  0.0647, -0.0291,  ...,  0.0434,  0.0115, -0.1420],\n",
       "         [ 0.0028,  0.0522, -0.0119,  ...,  0.0296,  0.0566, -0.1233],\n",
       "         [ 0.1021,  0.0503, -0.0248,  ...,  0.0112,  0.0512, -0.0909]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T09:59:12.176492Z",
     "start_time": "2026-02-27T09:59:12.132038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "mhla = MultiHeadLatentAttention(768, 768, 0.1, 8, latent_dim = 16)\n",
    "x = torch.randn((4, 16, 768))\n",
    "mhla(x, use_cache=True)\n"
   ],
   "id": "afd043747e25b1dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4875, -0.0411,  0.0817,  ...,  0.4159,  0.0751, -0.0362],\n",
       "         [-0.3747,  0.0040, -0.1376,  ...,  0.1695,  0.0344,  0.1456],\n",
       "         [-0.2558, -0.0190, -0.0046,  ..., -0.1732, -0.0314,  0.0327],\n",
       "         ...,\n",
       "         [-0.0434, -0.0430,  0.0536,  ..., -0.0190,  0.1086,  0.0185],\n",
       "         [-0.0076, -0.0088,  0.0619,  ...,  0.0151,  0.0752,  0.0256],\n",
       "         [-0.0299, -0.0438,  0.0467,  ..., -0.0277,  0.0474,  0.0263]],\n",
       "\n",
       "        [[-0.1220,  0.0608,  0.0130,  ..., -0.1628,  0.1217,  0.1792],\n",
       "         [-0.2831,  0.0726,  0.0782,  ...,  0.1233,  0.1931,  0.0833],\n",
       "         [-0.1875,  0.0378,  0.0440,  ...,  0.1351,  0.1351,  0.0959],\n",
       "         ...,\n",
       "         [ 0.0017,  0.0045, -0.0118,  ...,  0.0411,  0.0340,  0.0081],\n",
       "         [-0.0067,  0.0040,  0.0021,  ...,  0.0042,  0.0231, -0.0021],\n",
       "         [ 0.0036,  0.0009, -0.0563,  ...,  0.0039,  0.0498, -0.0169]],\n",
       "\n",
       "        [[ 0.0805,  0.0756, -0.2643,  ..., -0.1146,  0.0218,  0.1169],\n",
       "         [ 0.0141,  0.1417, -0.2870,  ...,  0.0772, -0.1675,  0.0676],\n",
       "         [-0.0129,  0.0579, -0.0195,  ..., -0.0410, -0.0302,  0.1583],\n",
       "         ...,\n",
       "         [ 0.0503,  0.0310, -0.0494,  ..., -0.0887,  0.0247,  0.0326],\n",
       "         [ 0.0169,  0.0429, -0.0409,  ..., -0.0932,  0.0406,  0.0613],\n",
       "         [ 0.0546,  0.0274, -0.0483,  ..., -0.0359,  0.0289,  0.0662]],\n",
       "\n",
       "        [[ 0.4103,  0.1271,  0.1097,  ..., -0.2564,  0.0220, -0.0653],\n",
       "         [ 0.1611,  0.0983,  0.2950,  ..., -0.0189,  0.3222, -0.0756],\n",
       "         [ 0.0055,  0.0563,  0.2137,  ..., -0.0778,  0.2371, -0.0616],\n",
       "         ...,\n",
       "         [ 0.0220,  0.0647, -0.0291,  ...,  0.0434,  0.0115, -0.1420],\n",
       "         [ 0.0028,  0.0522, -0.0119,  ...,  0.0296,  0.0566, -0.1233],\n",
       "         [ 0.1021,  0.0503, -0.0248,  ...,  0.0112,  0.0512, -0.0909]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "746727a77171407d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T09:52:52.502899Z",
     "start_time": "2026-02-27T09:52:52.494416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "63e4389d6958dd12",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T09:52:56.696066Z",
     "start_time": "2026-02-27T09:52:56.687612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ],
   "id": "4d683867dd88b487",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T09:53:01.168065Z",
     "start_time": "2026-02-27T09:53:01.159352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "403388f94a4c3fd8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T10:00:14.817888Z",
     "start_time": "2026-02-27T10:00:14.807675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadLatentAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"],\n",
    "            latent_dim = cfg[\"latent_dim\"],\n",
    "            )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        ####################################################\n",
    "        #  KV cache-related\n",
    "        x = self.att(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n"
   ],
   "id": "5a485916218bafc5",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T10:00:23.236211Z",
     "start_time": "2026-02-27T10:00:23.123531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "trb = TransformerBlock(cfg = GPT_CONFIG_124M)\n",
    "x = torch.randn((4, 16, 768))\n",
    "trb(x, use_cache=True)"
   ],
   "id": "9442230fa3f25833",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4884, -0.5535,  0.8693,  ...,  1.7824,  1.0540, -2.6442],\n",
       "         [ 0.5594, -1.2984,  2.3836,  ...,  1.4760,  1.0618, -0.2848],\n",
       "         [ 0.2841, -0.9488,  0.4599,  ...,  1.9853, -0.2283,  0.4034],\n",
       "         ...,\n",
       "         [ 0.0541,  0.4883,  1.0370,  ...,  0.6804, -1.3578, -0.1327],\n",
       "         [ 1.5739, -0.3282, -1.0375,  ...,  0.2705,  0.7439,  0.4705],\n",
       "         [-1.1375,  1.6443, -0.6123,  ...,  1.2042, -0.1471,  0.4203]],\n",
       "\n",
       "        [[ 0.9517, -0.1953,  1.6359,  ...,  0.5231, -0.4581,  1.1008],\n",
       "         [-0.7337,  0.9436, -1.3198,  ...,  0.9200,  0.8286,  1.5573],\n",
       "         [ 0.1246,  0.7043,  1.0642,  ..., -0.4129, -0.8991,  0.3662],\n",
       "         ...,\n",
       "         [ 1.6171,  0.7193, -1.8199,  ..., -1.2530, -1.2707, -0.8413],\n",
       "         [-0.1360, -0.6636, -0.8448,  ..., -0.4091, -0.9891,  0.3846],\n",
       "         [ 0.0416, -1.3906, -0.3378,  ...,  1.2605, -1.2222, -1.0058]],\n",
       "\n",
       "        [[ 0.3212,  0.4953, -0.9053,  ...,  1.1529, -0.7419,  0.4639],\n",
       "         [-1.0039,  0.2175, -0.5057,  ...,  1.5622, -1.4988,  0.3103],\n",
       "         [-1.4938, -0.7702, -1.0720,  ...,  0.4165, -0.1629,  0.0142],\n",
       "         ...,\n",
       "         [-0.7173, -0.7243, -0.4085,  ..., -0.2670,  1.2718,  1.6782],\n",
       "         [-0.0636,  0.3201,  0.2313,  ..., -0.5139, -0.9701, -0.5048],\n",
       "         [-0.5873, -1.0440, -0.2962,  ...,  0.4607, -0.1472,  0.7377]],\n",
       "\n",
       "        [[-2.1697,  1.4967, -0.1727,  ..., -0.2444, -0.3297,  0.2714],\n",
       "         [ 1.1868, -0.1199, -0.4900,  ...,  1.7388, -0.6566,  0.1601],\n",
       "         [ 0.4045,  0.6328, -1.0337,  ...,  1.7009, -0.7822, -0.6101],\n",
       "         ...,\n",
       "         [-0.2585, -0.5048,  0.1390,  ..., -0.1055, -0.2390,  0.9258],\n",
       "         [-0.0782,  0.0785, -0.5662,  ...,  1.5519, -0.0482, -0.5469],\n",
       "         [ 0.8161,  0.3169, -0.7112,  ...,  1.2050, -1.1238,  0.1004]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T10:01:40.622940Z",
     "start_time": "2026-02-27T10:01:40.619631Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ae8559a6422136e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T10:01:46.460757Z",
     "start_time": "2026-02-27T10:01:46.444037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # self.trf_blocks = nn.Sequential(\n",
    "        #    *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        ####################################################\n",
    "        #  KV cache-related\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.current_pos = 0\n",
    "        ####################################################\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        ####################################################\n",
    "        #  KV cache-related\n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(self.current_pos, self.current_pos + seq_len, device=in_idx.device, dtype=torch.long)\n",
    "            self.current_pos += seq_len\n",
    "        else:\n",
    "            pos_ids = torch.arange(0, seq_len, device=in_idx.device, dtype=torch.long)\n",
    "        pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)\n",
    "        ####################################################\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # x = self.trf_blocks(x)\n",
    "        ####################################################\n",
    "        # KV cache-related\n",
    "        for blk in self.trf_blocks:\n",
    "            x = blk(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    ####################################################\n",
    "    # KV cache-related\n",
    "    def reset_kv_cache(self):\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.att.reset_cache()\n",
    "        self.current_pos = 0\n",
    "    ####################################################"
   ],
   "id": "e12128c3dbbe6afa",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T10:01:57.478469Z",
     "start_time": "2026-02-27T10:01:48.101437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_context = \"Hello, I am\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.eval()"
   ],
   "id": "77b03085d3adbf1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (att): MultiHeadLatentAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_DKV): Linear(in_features=768, out_features=16, bias=False)\n",
       "        (W_UK): Linear(in_features=16, out_features=768, bias=False)\n",
       "        (W_UV): Linear(in_features=16, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T10:02:03.425465Z",
     "start_time": "2026-02-27T10:02:03.417868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n"
   ],
   "id": "ea08cbc1802e388d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T10:02:06.011844Z",
     "start_time": "2026-02-27T10:02:05.014180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = model(encoded_tensor)\n",
    "out"
   ],
   "id": "933d9d6732c2ad9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0762, -0.2383,  0.4707,  ...,  0.1816,  0.2393, -0.1211],\n",
       "         [ 0.1113,  0.5586, -0.6602,  ...,  0.6094, -0.0830, -0.1250],\n",
       "         [-0.3965, -0.2305, -0.2715,  ...,  0.1826,  0.2715,  0.9023],\n",
       "         [ 0.4707, -0.3457,  0.2637,  ...,  0.3477, -0.2832,  0.2246]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "98c5db79c938d4db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1362460eca32948c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
