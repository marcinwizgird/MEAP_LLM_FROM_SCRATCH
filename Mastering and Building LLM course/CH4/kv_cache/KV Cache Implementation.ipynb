{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "###Reading: https://magazine.sebastianraschka.com/p/coding-the-kv-cache-in-llms\n",
    "###Reading https://medium.com/data-scientists-diary/advanced-guide-to-using-nn-modulelist-in-pytorch-da4d49c109fc"
   ],
   "id": "4bafca0de4034b9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:36:05.200622Z",
     "start_time": "2026-02-15T15:36:01.967826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "2dd778756886fcf5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:36:05.218220Z",
     "start_time": "2026-02-15T15:36:05.209256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1),\n",
    "            persistent=False\n",
    "        )\n",
    "        ###NEW#####################################################################################\n",
    "        self.register_buffer(\"k_cache\", None, persistent = False)\n",
    "        self.register_buffer(\"v_cache\", None, persistent = False)\n",
    "        self.current_pos = 0\n",
    "        ###########################################################################################\n",
    "\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v = None, None\n",
    "\n",
    "    def forward(self, x, uses_cache=False):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        new_keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        new_values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        new_keys = new_keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        new_values = new_values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        ########################################################################################\n",
    "        #NEW\n",
    "\n",
    "        if uses_cache:\n",
    "            self.k_cache = torch.cat((self.k_cache, new_keys), dim=1) if self.k_cache is not None else new_keys\n",
    "            self.v_cache = torch.cat((self.v_cache, new_values), dim=1) if self.v_cache is not None else new_values\n",
    "            keys = self.k_cache\n",
    "            values = self.v_cache\n",
    "        else:\n",
    "            keys = new_keys\n",
    "            values = new_values\n",
    "        ###########################################################################################\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        ########################################################################################\n",
    "        #NEW\n",
    "\n",
    "        Q_num_tokens = queries.shape[-2]\n",
    "        K_num_tokens = keys.shape[-2]\n",
    "        if uses_cache:\n",
    "            mask_bool = self.mask.bool()[self.current_pos : self.current_pos + Q_num_tokens,:K_num_tokens]\n",
    "            self.current_pos += Q_num_tokens\n",
    "        else:\n",
    "            mask_bool = self.mask.bool()[:Q_num_tokens, :K_num_tokens]\n",
    "        ########################################################################################\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ],
   "id": "310eec71059afae2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:36:05.228162Z",
     "start_time": "2026-02-15T15:36:05.224341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "e0fdc2e7bd3cc1cd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:36:05.235859Z",
     "start_time": "2026-02-15T15:36:05.232289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ],
   "id": "1c1c168018843476",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:36:05.243879Z",
     "start_time": "2026-02-15T15:36:05.240256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "a2f5c0facf5e2542",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:36:05.254879Z",
     "start_time": "2026-02-15T15:36:05.248921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, uses_cache=False):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        x = self.att(x, uses_cache)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        ####################################################\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ],
   "id": "be771815fb77968",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:37:03.591509Z",
     "start_time": "2026-02-15T15:37:03.582823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        self.trf_blocks = nn.ModuleList( [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.current_pos = 0\n",
    "        ####################################################\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, uses_cache=False):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = None\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        if uses_cache:\n",
    "            pos_embeds = self.pos_emb(torch.arange(self.current_pos, self.current_pos + seq_len, device=in_idx.device ,dtype=torch.long))\n",
    "            self.current_pos += seq_len\n",
    "        else:\n",
    "            pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device ,dtype=torch.long))\n",
    "        ####################################################\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, uses_cache = uses_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    ####################################################\n",
    "    # NEW\n",
    "    def reset_kv_cache(self):\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.att.reset_cache()\n",
    "        self.current_pos = 0\n",
    "    ####################################################"
   ],
   "id": "e47290f4d20d71af",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:36:05.273481Z",
     "start_time": "2026-02-15T15:36:05.270427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ],
   "id": "10469ad4a2aba60d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:40:32.383264Z",
     "start_time": "2026-02-15T15:40:32.375855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_simple_cached(model, idx, max_new_tokens, context_size = None, uses_cache = False):\n",
    "    model.eval()\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    context_len = context_size or model.pos_emb.num_embeddings\n",
    "\n",
    "    if uses_cache:\n",
    "        #PREFILL\n",
    "        model.reset_kv_cache()\n",
    "        idx_cond = idx[:, -context_len:]\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Get the predictions\n",
    "            with torch.no_grad():\n",
    "                logits = model(idx_cond, uses_cache = True)\n",
    "\n",
    "            # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Get the idx of the vocab entry with the highest logits value\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "            idx_cond = idx_next\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    else:\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_len:]\n",
    "            with torch.no_grad():\n",
    "                logits = model(idx_cond, uses_cache = uses_cache)\n",
    "            logits = logits[:, -1, :]\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ],
   "id": "777014fb4ad7b838",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:40:32.852728Z",
     "start_time": "2026-02-15T15:40:32.843407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(uses_cache = False):\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    start_context = \"Hello, I am\"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    # token_ids = generate_text_simple(\n",
    "    #     model=model,\n",
    "    #     idx=encoded_tensor,\n",
    "    #     max_new_tokens=200,\n",
    "    #     context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    # )\n",
    "\n",
    "    ####################################################\n",
    "    # NEW\n",
    "    token_ids = generate_text_simple_cached(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=200,\n",
    "        uses_cache=uses_cache\n",
    "    )\n",
    "    ####################################################\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    total_time = time.time() - start\n",
    "\n",
    "    decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", token_ids)\n",
    "    print(\"Output length:\", len(token_ids[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "\n",
    "    print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "    print(f\"{int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "    if torch.cuda.is_available():\n",
    "        max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "        max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "        print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")"
   ],
   "id": "4b48e71dab16545",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:40:38.226421Z",
     "start_time": "2026-02-15T15:40:34.031696Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "414a6303085bb951",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657, 18631, 49188, 43312, 45933, 23154, 15983,\n",
      "         10345, 16369, 46214, 22954, 34674, 21100,  4743, 14056, 42526,  6459,\n",
      "         12799,  5734, 49274,   136, 49294, 42900, 21193, 20463,  1018,  7864,\n",
      "         13895, 27167, 12810, 25727, 14388,   985, 15797, 24440, 18557, 48625,\n",
      "         10579,  4007, 11895, 45365, 19051,  1355, 47705,  5120, 32858, 49293,\n",
      "          5141, 22900, 36570, 22215, 16369, 25803,  9254, 33694, 23188, 21624,\n",
      "         12696,  1697, 12315, 23338,  1361, 49487, 27970, 21641, 28170, 36226,\n",
      "          8980, 34715, 15683, 21370,   829, 41165, 19250, 40921, 47972, 29169,\n",
      "         17681, 13937,   719,  7781, 46519, 39685, 35637, 38254, 37355, 48054,\n",
      "          6960, 32389, 49945, 48307, 43363,  9451, 44360, 43623, 36233, 27350,\n",
      "         24597, 27334, 14212, 11243, 37010, 10723, 24492, 45795,  8216, 11362,\n",
      "         26754, 18517, 44987, 35224,  9535, 22574, 20579, 13134, 50072, 41511,\n",
      "         21603, 44476, 37708, 47619, 37179, 36111, 40412,  2529, 15569, 25531,\n",
      "         32132,  6450,  4333, 36408, 15648, 27025,  8393,   460,  5645,  8731,\n",
      "         29144, 30137, 32738, 37851,  5652, 22602, 36063, 15195, 19337, 25290,\n",
      "         22359, 14495, 46091, 31485,  9003, 42166, 33260,  5984, 20594, 28823,\n",
      "         29651, 43283,  1637,  3511, 43921, 28062,  3293, 20965, 38951, 34929,\n",
      "          6426, 41189, 36197, 50030, 37445, 50114,  6049, 21174, 30441, 49812,\n",
      "         35484, 28117,  4851, 17249, 17027, 17533, 14407, 25401, 41319,  9367,\n",
      "         28812,  6729, 43881, 38136]], device='cuda:0')\n",
      "Output length: 204\n",
      "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchangingVirgin embarrassedgl Truthfoundation challenges essence specifically Absent� 421 lov Castro Fewug wins Hus Golemllyyll Fisher sim trimュPrintOracle Five purpose FO treacherous grades Be1001 proceed rodsurated kid unpredictableFans goddess recountPet hint mustard178 Pul Hotelennpdfنreenammu312 paintings waived mL¯¯¯¯ruly thumbsvles complains Nashville Principleulla misuseフ Fel act scored782 consensual Cemetery Highly Quietceivable Young FirearmsAimEconomic awokebered triangles Sturgeon distressed ArriBal bash influential Pa PinballWatchPublished rehearsal tone GPUAshOctober overriding AVGtra heck username acted Cellular unob smoked Ludwig nailedeersMarco Folk aluminium inj aggressionvic yogurt sand Poweriannopoulos Rise Frances Exec can endsWind glove disparity fuse contestants AngelesutilColumb Stra shallow WS tumor peersassian clauses airportBenef Pug AD similarities commentator morale obnoxious money yourselfPolitics feder substotorCollegekeye revenue moistur allergy glared Kosovo mainline severeDan scrutin aph bananaseenthFClatedzi bored Phantom OTHERazel scanemployment snow fuelingeworld\n",
      "\n",
      "Time: 3.15 sec\n",
      "64 tokens/sec\n",
      "Max memory allocated: 1.40 GB\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T15:40:41.636111Z",
     "start_time": "2026-02-15T15:40:38.243763Z"
    }
   },
   "cell_type": "code",
   "source": "main(uses_cache=True)",
   "id": "8536583e197a018f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657, 18631, 49188, 43312, 45933, 23154, 15983,\n",
      "         10345, 16369, 46214, 22954, 34674, 21100,  4743, 14056, 42526,  6459,\n",
      "         12799,  5734, 49274,   136, 49294, 42900, 21193, 20463,  1018,  7864,\n",
      "         13895, 27167, 12810, 25727, 14388,   985, 15797, 24440, 18557, 48625,\n",
      "         10579,  4007, 11895, 45365, 19051,  1355, 47705,  5120, 32858, 49293,\n",
      "          5141, 22900, 36570, 22215, 16369, 25803,  9254, 33694, 23188, 21624,\n",
      "         12696,  1697, 12315, 23338,  1361, 49487, 27970, 21641, 28170, 36226,\n",
      "          8980, 34715, 15683, 21370,   829, 41165, 19250, 40921, 47972, 29169,\n",
      "         17681, 13937,   719,  7781, 46519, 39685, 35637, 38254, 37355, 48054,\n",
      "          6960, 32389, 49945, 48307, 43363,  9451, 44360, 43623, 36233, 27350,\n",
      "         24597, 27334, 14212, 11243, 37010, 10723, 24492, 45795,  8216, 11362,\n",
      "         26754, 18517, 44987, 35224,  9535, 22574, 20579, 13134, 50072, 41511,\n",
      "         21603, 44476, 37708, 47619, 37179, 36111, 40412,  2529, 15569, 25531,\n",
      "         32132,  6450,  4333, 36408, 15648, 27025,  8393,   460,  5645,  8731,\n",
      "         29144, 30137, 32738, 37851,  5652, 22602, 36063, 15195, 19337, 25290,\n",
      "         22359, 14495, 46091, 31485,  9003, 42166, 33260,  5984, 20594, 28823,\n",
      "         29651, 43283,  1637,  3511, 43921, 28062,  3293, 20965, 38951, 34929,\n",
      "          6426, 41189, 36197, 50030, 37445, 50114,  6049, 21174, 30441, 49812,\n",
      "         35484, 28117,  4851, 17249, 17027, 17533, 14407, 25401, 41319,  9367,\n",
      "         28812,  6729, 43881, 38136]], device='cuda:0')\n",
      "Output length: 204\n",
      "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchangingVirgin embarrassedgl Truthfoundation challenges essence specifically Absent� 421 lov Castro Fewug wins Hus Golemllyyll Fisher sim trimュPrintOracle Five purpose FO treacherous grades Be1001 proceed rodsurated kid unpredictableFans goddess recountPet hint mustard178 Pul Hotelennpdfنreenammu312 paintings waived mL¯¯¯¯ruly thumbsvles complains Nashville Principleulla misuseフ Fel act scored782 consensual Cemetery Highly Quietceivable Young FirearmsAimEconomic awokebered triangles Sturgeon distressed ArriBal bash influential Pa PinballWatchPublished rehearsal tone GPUAshOctober overriding AVGtra heck username acted Cellular unob smoked Ludwig nailedeersMarco Folk aluminium inj aggressionvic yogurt sand Poweriannopoulos Rise Frances Exec can endsWind glove disparity fuse contestants AngelesutilColumb Stra shallow WS tumor peersassian clauses airportBenef Pug AD similarities commentator morale obnoxious money yourselfPolitics feder substotorCollegekeye revenue moistur allergy glared Kosovo mainline severeDan scrutin aph bananaseenthFClatedzi bored Phantom OTHERazel scanemployment snow fuelingeworld\n",
      "\n",
      "Time: 2.62 sec\n",
      "77 tokens/sec\n",
      "Max memory allocated: 1.40 GB\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f072df0d3996486"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
