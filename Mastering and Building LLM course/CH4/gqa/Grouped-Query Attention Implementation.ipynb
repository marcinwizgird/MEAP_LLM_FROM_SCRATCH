{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-19T10:07:55.640669Z",
     "start_time": "2026-02-19T10:07:50.514905Z"
    }
   },
   "source": [
    "import argparse\n",
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:44:11.040881Z",
     "start_time": "2026-02-19T18:44:11.036921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_kv_heads\": 4,         #\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }"
   ],
   "id": "432febf43fcad6ba",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:44:12.338372Z",
     "start_time": "2026-02-19T18:44:12.323976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, num_heads, num_kv_groups,  qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        self.W_query =  nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, self.num_kv_groups * self.head_dim, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, self.num_kv_groups * self.head_dim, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ####################################################\n",
    "        # KV cache-related code\n",
    "        self.register_buffer(\"cache_k\", None, persistent=False)\n",
    "        self.register_buffer(\"cache_v\", None, persistent=False)\n",
    "        self.ptr_current_pos = 0\n",
    "        ####################################################\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys_new = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        values_new = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys_base = keys_new.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values_base = values_new.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        ####################################################\n",
    "        # KV cache-related\n",
    "        if use_cache:\n",
    "            if self.cache_k is None:\n",
    "                self.cache_k, self.cache_v = keys_base, values_base\n",
    "            else:\n",
    "                self.cache_k = torch.cat([self.cache_k, keys_base], dim=1)\n",
    "                self.cache_v = torch.cat([self.cache_v, values_base], dim=1)\n",
    "            keys, values = self.cache_k, self.cache_v\n",
    "        else:\n",
    "            keys, values = keys_base, values_base\n",
    "        ####################################################\n",
    "        keys = torch.repeat_interleave(keys, self.group_size, dim = 1)\n",
    "        values = torch.repeat_interleave(values, self.group_size, dim = 1)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        print(f\"{queries.shape=}, {keys.shape=}, {values.shape=}\")\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "\n",
    "        ####################################################\n",
    "        # causal mask\n",
    "        num_tokens_Q = queries.shape[-2]\n",
    "        num_tokens_K = keys.shape[-2]\n",
    "        device = queries.device\n",
    "        if use_cache:\n",
    "            q_positions = torch.arange(\n",
    "                self.ptr_current_pos,\n",
    "                self.ptr_current_pos + num_tokens_Q,\n",
    "                device=device,\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "            self.ptr_current_pos += num_tokens_Q\n",
    "        else:\n",
    "            q_positions = torch.arange(num_tokens_Q, device=device, dtype=torch.long)\n",
    "            self.ptr_current_pos = 0\n",
    "        k_positions = torch.arange(num_tokens_K, device=device, dtype=torch.long)\n",
    "        mask_bool = q_positions.unsqueeze(-1) < k_positions.unsqueeze(0)\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v = None, None\n",
    "        self.ptr_current_pos = 0"
   ],
   "id": "a0e8940117ee637c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:28:34.370952Z",
     "start_time": "2026-02-19T18:28:34.326310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "gqa = GroupedQueryAttention(8, 8, 0.1, 4, 2)\n",
    "x = torch.randn((4, 16, 8))\n",
    "gqa(x, use_cache=True)\n"
   ],
   "id": "afd043747e25b1dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape=torch.Size([4, 4, 16, 2]), keys.shape=torch.Size([4, 4, 16, 2]), values.shape=torch.Size([4, 4, 16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2920,  0.0703,  0.2762, -0.2524, -0.5075, -0.1266,  0.3727,\n",
       "           0.6040],\n",
       "         [ 0.2058,  0.1342, -0.0123, -0.4686, -0.2325, -0.2556,  0.4262,\n",
       "           0.5283],\n",
       "         [ 0.2384,  0.1692, -0.0439, -0.3199, -0.2742, -0.2083,  0.4479,\n",
       "           0.4003],\n",
       "         [ 0.3033,  0.1678, -0.0886, -0.1825,  0.0342, -0.1634,  0.2930,\n",
       "           0.3305],\n",
       "         [ 0.1602,  0.2293, -0.1543, -0.1237,  0.0383, -0.2116,  0.2886,\n",
       "           0.1665],\n",
       "         [ 0.0709,  0.3052, -0.2888, -0.2147,  0.2245, -0.3072,  0.2445,\n",
       "           0.1013],\n",
       "         [ 0.0150,  0.3179, -0.2837, -0.0542,  0.2898, -0.2506,  0.1499,\n",
       "          -0.0657],\n",
       "         [ 0.0024,  0.3060, -0.3070,  0.0418,  0.3283, -0.2031,  0.1362,\n",
       "          -0.1713],\n",
       "         [ 0.0414,  0.2747, -0.2766, -0.0530,  0.3010, -0.3000,  0.2065,\n",
       "           0.0594],\n",
       "         [ 0.0709,  0.2530, -0.2590, -0.0353,  0.4087, -0.3184,  0.1563,\n",
       "           0.0994],\n",
       "         [-0.0824,  0.4708, -0.5199,  0.0191,  0.3002, -0.1633,  0.1479,\n",
       "          -0.4463],\n",
       "         [ 0.0313,  0.3531, -0.3150, -0.0389,  0.2946, -0.3027,  0.1831,\n",
       "          -0.0799],\n",
       "         [ 0.1655,  0.1961, -0.2395, -0.1173,  0.0936, -0.3051,  0.4186,\n",
       "           0.2420],\n",
       "         [ 0.0376,  0.4306, -0.3702,  0.0986,  0.2059, -0.2571,  0.2113,\n",
       "          -0.1442],\n",
       "         [ 0.1490,  0.2697, -0.2561, -0.0536,  0.1457, -0.2006,  0.2737,\n",
       "           0.0591],\n",
       "         [ 0.0998,  0.1061, -0.3172, -0.1801,  0.0176, -0.1472,  0.4921,\n",
       "           0.0751]],\n",
       "\n",
       "        [[-0.2579,  0.4417, -0.5868, -0.6445,  0.1894, -0.4145,  0.3448,\n",
       "          -0.1703],\n",
       "         [-0.3278,  0.6151, -0.7373, -0.2369,  0.3289, -0.2423,  0.1520,\n",
       "          -0.6913],\n",
       "         [ 0.1226,  0.2951, -0.2051, -0.2994, -0.0874, -0.1792,  0.3072,\n",
       "           0.0875],\n",
       "         [-0.1357,  0.3555, -0.3640, -0.1092, -0.0902,  0.0416,  0.1952,\n",
       "          -0.4210],\n",
       "         [-0.1324,  0.4039, -0.4177,  0.0670, -0.0214, -0.0063,  0.2145,\n",
       "          -0.5296],\n",
       "         [ 0.0723,  0.3851, -0.2090,  0.0780,  0.3449, -0.1977, -0.0159,\n",
       "          -0.2317],\n",
       "         [-0.1265,  0.4373, -0.5493,  0.0702,  0.0889, -0.1517,  0.3342,\n",
       "          -0.4610],\n",
       "         [ 0.0545,  0.3761, -0.3253, -0.0812,  0.0236, -0.1316,  0.2508,\n",
       "          -0.1826],\n",
       "         [-0.0799,  0.4839, -0.4590, -0.0008,  0.1870, -0.1275,  0.1306,\n",
       "          -0.4488],\n",
       "         [-0.0984,  0.4454, -0.4632,  0.0064,  0.1578, -0.0744,  0.1470,\n",
       "          -0.4491],\n",
       "         [-0.0537,  0.4433, -0.3977,  0.0337,  0.2598, -0.1706,  0.1027,\n",
       "          -0.3618],\n",
       "         [-0.0638,  0.4600, -0.4650, -0.0444,  0.0323, -0.1378,  0.2669,\n",
       "          -0.3787],\n",
       "         [ 0.0221,  0.4075, -0.3801, -0.1660,  0.1275, -0.1755,  0.2135,\n",
       "          -0.1969],\n",
       "         [-0.0549,  0.3833, -0.4359, -0.1491,  0.0285, -0.0909,  0.2706,\n",
       "          -0.3579],\n",
       "         [-0.0712,  0.4192, -0.4151, -0.1516, -0.0462, -0.2344,  0.3591,\n",
       "          -0.2143],\n",
       "         [-0.0292,  0.3814, -0.3963, -0.2043, -0.0678, -0.2428,  0.3981,\n",
       "          -0.1083]],\n",
       "\n",
       "        [[ 0.6783,  0.3081, -0.4164,  0.2789,  0.5620,  0.2740,  0.0784,\n",
       "          -0.0420],\n",
       "         [ 0.1423,  0.1348, -0.2759,  0.1169,  0.3456, -0.4023,  0.3773,\n",
       "           0.1179],\n",
       "         [ 0.1393,  0.4169, -0.4749,  0.2111,  0.5428, -0.0827,  0.0573,\n",
       "          -0.3195],\n",
       "         [ 0.2522,  0.3318, -0.2813,  0.2794,  0.4125, -0.2257,  0.1374,\n",
       "          -0.0238],\n",
       "         [ 0.2149,  0.2671, -0.2472,  0.1153,  0.1503, -0.1698,  0.2886,\n",
       "           0.0896],\n",
       "         [ 0.1466,  0.3489, -0.2518,  0.1541,  0.2501, -0.1176,  0.1151,\n",
       "          -0.1491],\n",
       "         [ 0.1770,  0.2351, -0.1788, -0.0365,  0.0887, -0.2787,  0.3238,\n",
       "           0.2491],\n",
       "         [ 0.2238,  0.2481, -0.2978,  0.2000,  0.3643, -0.1628,  0.2084,\n",
       "          -0.0357],\n",
       "         [ 0.1605,  0.3340, -0.2949,  0.1683,  0.3274, -0.1899,  0.1468,\n",
       "          -0.0480],\n",
       "         [ 0.1552,  0.2179, -0.2105, -0.2012,  0.1297, -0.1916,  0.2779,\n",
       "           0.1651],\n",
       "         [ 0.2130,  0.1870, -0.2357,  0.0660,  0.2470, -0.2661,  0.3174,\n",
       "           0.2171],\n",
       "         [ 0.2913,  0.1776, -0.1381, -0.1149,  0.0020, -0.2732,  0.4180,\n",
       "           0.3810],\n",
       "         [ 0.3298,  0.2021, -0.1185, -0.1579,  0.1720, -0.2167,  0.2426,\n",
       "           0.3971],\n",
       "         [ 0.1942,  0.2069, -0.2366, -0.1738,  0.1827, -0.4304,  0.4130,\n",
       "           0.3890],\n",
       "         [ 0.3160,  0.2339, -0.1798, -0.1124,  0.1507, -0.2122,  0.2851,\n",
       "           0.2714],\n",
       "         [ 0.2716,  0.2006, -0.2558, -0.1247,  0.2436, -0.2551,  0.3231,\n",
       "           0.2991]],\n",
       "\n",
       "        [[-0.1538,  0.2839, -0.1631, -0.7108, -0.2849, -0.3739,  0.4222,\n",
       "           0.1610],\n",
       "         [ 0.0387,  0.2694, -0.1773, -0.5127, -0.2965, -0.2666,  0.4527,\n",
       "           0.2210],\n",
       "         [-0.1194,  0.4031, -0.3791, -0.1817,  0.1991, -0.1840,  0.1342,\n",
       "          -0.2671],\n",
       "         [-0.1519,  0.4698, -0.6100,  0.2563,  0.6604, -0.3757,  0.1136,\n",
       "          -0.4977],\n",
       "         [-0.0817,  0.4368, -0.3968,  0.0194,  0.2722, -0.2336,  0.1330,\n",
       "          -0.3516],\n",
       "         [-0.1288,  0.4838, -0.4785,  0.0863,  0.3571, -0.2351,  0.1014,\n",
       "          -0.4257],\n",
       "         [-0.1556,  0.5039, -0.5174,  0.0054,  0.4006, -0.2622,  0.0867,\n",
       "          -0.4192],\n",
       "         [-0.1448,  0.5610, -0.4914,  0.0341,  0.3401, -0.2549,  0.0588,\n",
       "          -0.4193],\n",
       "         [-0.0917,  0.4317, -0.4042, -0.0264,  0.2093, -0.1667,  0.1344,\n",
       "          -0.3254],\n",
       "         [-0.0718,  0.3889, -0.4346,  0.0200,  0.2230, -0.1591,  0.1941,\n",
       "          -0.3621],\n",
       "         [-0.0158,  0.3612, -0.3772, -0.0210,  0.2768, -0.2817,  0.2158,\n",
       "          -0.1481],\n",
       "         [-0.0215,  0.4086, -0.3804,  0.0494,  0.2720, -0.2103,  0.1380,\n",
       "          -0.2098],\n",
       "         [ 0.0119,  0.3230, -0.3773, -0.0129,  0.2122, -0.1900,  0.2519,\n",
       "          -0.1942],\n",
       "         [ 0.0580,  0.3847, -0.2866, -0.0117,  0.1488, -0.2185,  0.1892,\n",
       "          -0.1127],\n",
       "         [ 0.0369,  0.3498, -0.3701, -0.0198,  0.2016, -0.1102,  0.1909,\n",
       "          -0.2476],\n",
       "         [ 0.0561,  0.4669, -0.3589,  0.0772,  0.2447, -0.2566,  0.1527,\n",
       "          -0.1844]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:29:50.024934Z",
     "start_time": "2026-02-19T18:29:50.013648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "63e4389d6958dd12",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:29:50.785307Z",
     "start_time": "2026-02-19T18:29:50.779383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ],
   "id": "4d683867dd88b487",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:29:52.600351Z",
     "start_time": "2026-02-19T18:29:52.591171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "403388f94a4c3fd8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:43:05.950193Z",
     "start_time": "2026-02-19T18:43:05.943312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        ####################################################\n",
    "        #  KV cache-related\n",
    "        x = self.att(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n"
   ],
   "id": "5a485916218bafc5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:46:15.293036Z",
     "start_time": "2026-02-19T18:46:15.207807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "trb = TransformerBlock(cfg = GPT_CONFIG_124M)\n",
    "x = torch.randn((4, 16, 768))\n",
    "trb(x, use_cache=True)"
   ],
   "id": "9442230fa3f25833",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape=torch.Size([4, 12, 16, 64]), keys.shape=torch.Size([4, 12, 16, 64]), values.shape=torch.Size([4, 12, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.6094e-01, -1.2579e+00, -9.9960e-01,  ..., -9.0474e-01,\n",
       "          -6.4033e-01, -6.8059e-01],\n",
       "         [-1.1922e+00, -2.3845e+00, -2.7026e-02,  ...,  1.3562e+00,\n",
       "          -1.1489e+00, -1.3625e+00],\n",
       "         [-1.5047e-01, -5.6593e-01, -1.2998e+00,  ...,  4.3289e-01,\n",
       "          -7.0673e-01, -3.1272e-01],\n",
       "         ...,\n",
       "         [ 7.3964e-01, -8.9510e-01, -3.6306e-01,  ..., -1.3957e+00,\n",
       "          -4.7772e-01,  2.5455e+00],\n",
       "         [ 1.8428e+00, -1.9542e+00, -8.9446e-01,  ..., -1.4467e+00,\n",
       "          -3.8625e-01,  4.6340e-01],\n",
       "         [-1.1149e+00, -5.9360e-01, -3.8470e-01,  ...,  3.4420e-01,\n",
       "          -1.3897e+00,  9.8030e-01]],\n",
       "\n",
       "        [[-1.2351e-01,  6.8971e-01,  1.2014e-01,  ...,  2.8277e-01,\n",
       "           1.5974e+00,  6.3322e-01],\n",
       "         [ 9.2666e-02,  1.7620e+00,  2.4819e-01,  ...,  8.9675e-01,\n",
       "           1.7424e-01, -3.7959e-01],\n",
       "         [-3.2083e-01,  3.5708e-01, -6.6473e-01,  ...,  2.0882e-01,\n",
       "           6.1683e-01,  3.0177e+00],\n",
       "         ...,\n",
       "         [-7.0430e-01, -1.3407e+00,  1.0216e-03,  ...,  1.1337e+00,\n",
       "           5.5682e-01,  3.2826e-02],\n",
       "         [ 7.4224e-01,  1.0047e-01,  1.9924e+00,  ...,  1.1720e-01,\n",
       "          -5.4341e-01,  6.3977e-01],\n",
       "         [-8.0057e-01, -6.6555e-02, -5.0618e-01,  ...,  7.4727e-01,\n",
       "           2.3030e+00, -3.1724e-01]],\n",
       "\n",
       "        [[-6.0344e-01,  8.5225e-01, -6.6408e-01,  ..., -5.4395e-01,\n",
       "          -1.4641e-01, -3.1094e+00],\n",
       "         [ 5.4498e-01, -7.9496e-01, -3.1142e-01,  ..., -3.7453e-01,\n",
       "          -6.0414e-01, -1.1284e+00],\n",
       "         [-2.5147e-01, -5.1564e-01, -1.6185e+00,  ..., -9.1681e-01,\n",
       "          -8.3800e-02,  1.5651e+00],\n",
       "         ...,\n",
       "         [ 4.0948e-01,  1.9439e+00, -1.4046e+00,  ...,  2.5496e-01,\n",
       "          -4.2377e-01, -4.0811e-01],\n",
       "         [-9.1147e-01, -1.5149e+00, -4.0929e-01,  ..., -2.1634e+00,\n",
       "           4.9890e-01, -1.6060e-01],\n",
       "         [-3.7690e-01, -7.7462e-01, -4.8545e-01,  ...,  6.9100e-01,\n",
       "          -9.3427e-01, -6.1731e-01]],\n",
       "\n",
       "        [[ 1.3462e+00, -1.6073e-01,  8.4987e-01,  ...,  3.4845e-01,\n",
       "          -8.4410e-01, -1.5949e+00],\n",
       "         [ 2.8737e-01,  2.3067e+00,  1.5390e-01,  ...,  1.6352e-01,\n",
       "          -6.4952e-01, -5.5308e-01],\n",
       "         [-1.0644e+00,  1.3791e+00, -7.8620e-01,  ...,  9.2255e-01,\n",
       "          -7.4661e-01, -1.1120e-01],\n",
       "         ...,\n",
       "         [ 1.3439e+00, -9.0915e-01, -1.4526e+00,  ..., -4.9170e-01,\n",
       "           1.4025e-01,  2.0692e+00],\n",
       "         [ 7.2187e-02, -2.2990e-01, -6.9597e-01,  ...,  1.5190e+00,\n",
       "           8.3442e-01,  3.6245e-01],\n",
       "         [-4.6703e-01, -1.1528e+00, -5.1070e-02,  ..., -1.2143e+00,\n",
       "           6.0902e-01, -1.4588e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        ####################################################\n",
    "        #  KV cache-related\n",
    "        x = self.att(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ],
   "id": "84bd7d3dd9d9d56c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T18:47:12.800259Z",
     "start_time": "2026-02-19T18:47:12.734051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "trb = TransformerBlock(cfg = GPT_CONFIG_124M)\n",
    "x = torch.randn((4, 16, 768))\n",
    "trb(x, use_cache=True)"
   ],
   "id": "78e22e4d8e6f7bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape=torch.Size([4, 12, 16, 64]), keys.shape=torch.Size([4, 12, 16, 64]), values.shape=torch.Size([4, 12, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.6094e-01, -1.2579e+00, -9.9960e-01,  ..., -9.0474e-01,\n",
       "          -6.4033e-01, -6.8059e-01],\n",
       "         [-1.1922e+00, -2.3845e+00, -2.7026e-02,  ...,  1.3562e+00,\n",
       "          -1.1489e+00, -1.3625e+00],\n",
       "         [-1.5047e-01, -5.6593e-01, -1.2998e+00,  ...,  4.3289e-01,\n",
       "          -7.0673e-01, -3.1272e-01],\n",
       "         ...,\n",
       "         [ 7.3964e-01, -8.9510e-01, -3.6306e-01,  ..., -1.3957e+00,\n",
       "          -4.7772e-01,  2.5455e+00],\n",
       "         [ 1.8428e+00, -1.9542e+00, -8.9446e-01,  ..., -1.4467e+00,\n",
       "          -3.8625e-01,  4.6340e-01],\n",
       "         [-1.1149e+00, -5.9360e-01, -3.8470e-01,  ...,  3.4420e-01,\n",
       "          -1.3897e+00,  9.8030e-01]],\n",
       "\n",
       "        [[-1.2351e-01,  6.8971e-01,  1.2014e-01,  ...,  2.8277e-01,\n",
       "           1.5974e+00,  6.3322e-01],\n",
       "         [ 9.2666e-02,  1.7620e+00,  2.4819e-01,  ...,  8.9675e-01,\n",
       "           1.7424e-01, -3.7959e-01],\n",
       "         [-3.2083e-01,  3.5708e-01, -6.6473e-01,  ...,  2.0882e-01,\n",
       "           6.1683e-01,  3.0177e+00],\n",
       "         ...,\n",
       "         [-7.0430e-01, -1.3407e+00,  1.0216e-03,  ...,  1.1337e+00,\n",
       "           5.5682e-01,  3.2826e-02],\n",
       "         [ 7.4224e-01,  1.0047e-01,  1.9924e+00,  ...,  1.1720e-01,\n",
       "          -5.4341e-01,  6.3977e-01],\n",
       "         [-8.0057e-01, -6.6555e-02, -5.0618e-01,  ...,  7.4727e-01,\n",
       "           2.3030e+00, -3.1724e-01]],\n",
       "\n",
       "        [[-6.0344e-01,  8.5225e-01, -6.6408e-01,  ..., -5.4395e-01,\n",
       "          -1.4641e-01, -3.1094e+00],\n",
       "         [ 5.4498e-01, -7.9496e-01, -3.1142e-01,  ..., -3.7453e-01,\n",
       "          -6.0414e-01, -1.1284e+00],\n",
       "         [-2.5147e-01, -5.1564e-01, -1.6185e+00,  ..., -9.1681e-01,\n",
       "          -8.3800e-02,  1.5651e+00],\n",
       "         ...,\n",
       "         [ 4.0948e-01,  1.9439e+00, -1.4046e+00,  ...,  2.5496e-01,\n",
       "          -4.2377e-01, -4.0811e-01],\n",
       "         [-9.1147e-01, -1.5149e+00, -4.0929e-01,  ..., -2.1634e+00,\n",
       "           4.9890e-01, -1.6060e-01],\n",
       "         [-3.7690e-01, -7.7462e-01, -4.8545e-01,  ...,  6.9100e-01,\n",
       "          -9.3427e-01, -6.1731e-01]],\n",
       "\n",
       "        [[ 1.3462e+00, -1.6073e-01,  8.4987e-01,  ...,  3.4845e-01,\n",
       "          -8.4410e-01, -1.5949e+00],\n",
       "         [ 2.8737e-01,  2.3067e+00,  1.5390e-01,  ...,  1.6352e-01,\n",
       "          -6.4952e-01, -5.5308e-01],\n",
       "         [-1.0644e+00,  1.3791e+00, -7.8620e-01,  ...,  9.2255e-01,\n",
       "          -7.4661e-01, -1.1120e-01],\n",
       "         ...,\n",
       "         [ 1.3439e+00, -9.0915e-01, -1.4526e+00,  ..., -4.9170e-01,\n",
       "           1.4025e-01,  2.0692e+00],\n",
       "         [ 7.2187e-02, -2.2990e-01, -6.9597e-01,  ...,  1.5190e+00,\n",
       "           8.3442e-01,  3.6245e-01],\n",
       "         [-4.6703e-01, -1.1528e+00, -5.1070e-02,  ..., -1.2143e+00,\n",
       "           6.0902e-01, -1.4588e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae8559a6422136e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # self.trf_blocks = nn.Sequential(\n",
    "        #    *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        ####################################################\n",
    "        #  KV cache-related\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.current_pos = 0\n",
    "        ####################################################\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        ####################################################\n",
    "        #  KV cache-related\n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(self.current_pos, self.current_pos + seq_len, device=in_idx.device, dtype=torch.long)\n",
    "            self.current_pos += seq_len\n",
    "        else:\n",
    "            pos_ids = torch.arange(0, seq_len, device=in_idx.device, dtype=torch.long)\n",
    "        pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)\n",
    "        ####################################################\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # x = self.trf_blocks(x)\n",
    "        ####################################################\n",
    "        # KV cache-related\n",
    "        for blk in self.trf_blocks:\n",
    "            x = blk(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    ####################################################\n",
    "    # KV cache-related\n",
    "    def reset_kv_cache(self):\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.att.reset_cache()\n",
    "        self.current_pos = 0\n",
    "    ####################################################"
   ],
   "id": "e12128c3dbbe6afa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
